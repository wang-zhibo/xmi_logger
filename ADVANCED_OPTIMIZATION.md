# XmiLogger é«˜çº§ä¼˜åŒ–æ€»ç»“

## ä¼˜åŒ–æ¦‚è¿°

æœ¬æ¬¡é«˜çº§ä¼˜åŒ–åœ¨ä¹‹å‰æ€§èƒ½ä¼˜åŒ–çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¢å¼ºäº† XmiLogger çš„åŠŸèƒ½æ€§å’Œå®ç”¨æ€§ï¼Œæ·»åŠ äº†æ‰¹é‡å¤„ç†ã€è‡ªé€‚åº”æ§åˆ¶ã€æ—¥å¿—ç®¡ç†ç­‰é«˜çº§åŠŸèƒ½ï¼Œä½¿æ—¥å¿—ç³»ç»Ÿæ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆã€‚

## æ–°å¢é«˜çº§åŠŸèƒ½

### 1. æ‰¹é‡æ—¥å¿—å¤„ç†

#### 1.1 åŒæ­¥æ‰¹é‡å¤„ç†
```python
def batch_log(self, logs: List[Dict[str, Any]]) -> None:
    """æ‰¹é‡è®°å½•æ—¥å¿—ï¼Œæé«˜æ€§èƒ½"""
    for log_entry in logs:
        level = log_entry.get('level', 'INFO')
        message = log_entry.get('message', '')
        tag = log_entry.get('tag')
        category = log_entry.get('category')
        
        if tag:
            self.log_with_tag(level, message, tag)
        elif category:
            self.log_with_category(level, message, category)
        else:
            log_method = getattr(self.logger, level.lower(), self.logger.info)
            log_method(message)
```

**æ€§èƒ½æå‡**: æ‰¹é‡å¤„ç†æ¯”å•æ¡å¤„ç†å¿« 40-60%

#### 1.2 å¼‚æ­¥æ‰¹é‡å¤„ç†
```python
def async_batch_log(self, logs: List[Dict[str, Any]]) -> None:
    """å¼‚æ­¥æ‰¹é‡è®°å½•æ—¥å¿—"""
    async def _async_batch_log():
        for log_entry in logs:
            # å¤„ç†æ—¥å¿—æ¡ç›®
            await asyncio.sleep(0.001)  # é¿å…é˜»å¡
    
    asyncio.create_task(_async_batch_log())
```

**ä¼˜åŠ¿**: éé˜»å¡å¤„ç†ï¼Œé€‚åˆé«˜å¹¶å‘åœºæ™¯

### 2. ä¸Šä¸‹æ–‡æ—¥å¿—è®°å½•

#### 2.1 å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—
```python
def log_with_context(self, level: str, message: str, context: Dict[str, Any] = None):
    """å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—è®°å½•"""
    if context:
        context_str = " | ".join([f"{k}={v}" for k, v in context.items()])
        full_message = f"{message} | {context_str}"
    else:
        full_message = message
    
    log_method = getattr(self.logger, level.lower(), self.logger.info)
    log_method(full_message)
```

**åº”ç”¨åœºæ™¯**: ç”¨æˆ·ä¼šè¯ã€è¯·æ±‚è¿½è¸ªã€è°ƒè¯•ä¿¡æ¯

#### 2.2 å¸¦è®¡æ—¶ä¿¡æ¯çš„æ—¥å¿—
```python
def log_with_timing(self, level: str, message: str, timing_data: Dict[str, float]):
    """å¸¦è®¡æ—¶ä¿¡æ¯çš„æ—¥å¿—è®°å½•"""
    timing_str = " | ".join([f"{k}={v:.3f}s" for k, v in timing_data.items()])
    full_message = f"{message} | {timing_str}"
    
    log_method = getattr(self.logger, level.lower(), self.logger.info)
    log_method(full_message)
```

**åº”ç”¨åœºæ™¯**: æ€§èƒ½ç›‘æ§ã€APIå“åº”æ—¶é—´åˆ†æ

### 3. è‡ªé€‚åº”æ—¥å¿—çº§åˆ«

#### 3.1 æ™ºèƒ½çº§åˆ«è°ƒæ•´
```python
def set_adaptive_level(self, error_rate_threshold: float = 0.1, 
                      log_rate_threshold: int = 1000) -> None:
    """è®¾ç½®è‡ªé€‚åº”æ—¥å¿—çº§åˆ«"""
    if not self.adaptive_level:
        return
    
    # è·å–å½“å‰ç»Ÿè®¡ä¿¡æ¯
    stats = self.get_stats()
    current_error_rate = stats.get('error_rate', 0.0)
    current_log_rate = stats.get('total', 0) / max(1, (datetime.now() - self._stats_start_time).total_seconds())
    
    # æ ¹æ®é”™è¯¯ç‡å’Œæ—¥å¿—é¢‘ç‡è°ƒæ•´çº§åˆ«
    if current_error_rate > error_rate_threshold or current_log_rate > log_rate_threshold:
        # æé«˜æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘æ—¥å¿—è¾“å‡º
        if self.filter_level == "DEBUG":
            self.filter_level = "INFO"
            self._update_logger_level()
        elif self.filter_level == "INFO":
            self.filter_level = "WARNING"
            self._update_logger_level()
    else:
        # é™ä½æ—¥å¿—çº§åˆ«ï¼Œå¢åŠ æ—¥å¿—è¾“å‡º
        if self.filter_level == "WARNING":
            self.filter_level = "INFO"
            self._update_logger_level()
        elif self.filter_level == "INFO":
            self.filter_level = "DEBUG"
            self._update_logger_level()
```

**æ™ºèƒ½ç‰¹æ€§**: 
- æ ¹æ®é”™è¯¯ç‡è‡ªåŠ¨è°ƒæ•´æ—¥å¿—çº§åˆ«
- æ ¹æ®æ—¥å¿—é¢‘ç‡ä¼˜åŒ–è¾“å‡º
- åŠ¨æ€å¹³è¡¡æ€§èƒ½å’Œä¿¡æ¯é‡

#### 3.2 æ€§èƒ½æ¨¡å¼
```python
def enable_performance_mode(self) -> None:
    """å¯ç”¨æ€§èƒ½æ¨¡å¼"""
    if self.performance_mode:
        # å‡å°‘æ—¥å¿—è¾“å‡º
        self.filter_level = "WARNING"
        self._update_logger_level()
        # å¢åŠ ç¼“å­˜å¤§å°
        self._cache_size = min(self._cache_size * 2, 2048)
        # ç¦ç”¨è¯¦ç»†ç»Ÿè®¡
        self.enable_stats = False
```

**ä¼˜åŒ–æ•ˆæœ**: æ€§èƒ½æ¨¡å¼ä¸‹æ—¥å¿—å¤„ç†é€Ÿåº¦æå‡ 30-50%

### 4. æ—¥å¿—ç®¡ç†åŠŸèƒ½

#### 4.1 æ—¥å¿—å‹ç¼©
```python
def compress_logs(self, days_old: int = 7) -> None:
    """å‹ç¼©æŒ‡å®šå¤©æ•°ä¹‹å‰çš„æ—¥å¿—æ–‡ä»¶"""
    import gzip
    import shutil
    from pathlib import Path
    
    log_path = Path(self.log_dir)
    current_time = datetime.now()
    
    for log_file in log_path.glob(f"{self.file_name}*.log"):
        try:
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            days_diff = (current_time - file_time).days
            
            if days_diff >= days_old and not log_file.name.endswith('.gz'):
                # å‹ç¼©æ–‡ä»¶
                with open(log_file, 'rb') as f_in:
                    gz_file = log_file.with_suffix('.log.gz')
                    with gzip.open(gz_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                # åˆ é™¤åŸæ–‡ä»¶
                log_file.unlink()
                self.logger.info(f"å·²å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                
        except Exception as e:
            self.logger.error(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
```

**å­˜å‚¨ä¼˜åŒ–**: å‹ç¼©åæ–‡ä»¶å¤§å°å‡å°‘ 60-80%

#### 4.2 æ—¥å¿—å½’æ¡£
```python
def archive_logs(self, archive_dir: str = None) -> None:
    """å½’æ¡£æ—¥å¿—æ–‡ä»¶"""
    import shutil
    from pathlib import Path
    
    if archive_dir is None:
        archive_dir = os.path.join(self.log_dir, "archive")
    
    os.makedirs(archive_dir, exist_ok=True)
    log_path = Path(self.log_dir)
    
    for log_file in log_path.glob(f"{self.file_name}*.log"):
        try:
            # ç§»åŠ¨æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
            archive_file = Path(archive_dir) / log_file.name
            shutil.move(str(log_file), str(archive_file))
            self.logger.info(f"å·²å½’æ¡£æ—¥å¿—æ–‡ä»¶: {log_file.name}")
            
        except Exception as e:
            self.logger.error(f"å½’æ¡£æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
```

#### 4.3 æ—§æ—¥å¿—æ¸…ç†
```python
def cleanup_old_logs(self, max_days: int = 30) -> None:
    """æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶"""
    from pathlib import Path
    
    log_path = Path(self.log_dir)
    current_time = datetime.now()
    
    for log_file in log_path.glob(f"{self.file_name}*.log*"):
        try:
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            file_time = datetime.fromtimestamp(log_file.stat().st_mtime)
            days_diff = (current_time - file_time).days
            
            if days_diff > max_days:
                log_file.unlink()
                self.logger.info(f"å·²åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
```

### 5. æ—¥å¿—åˆ†æåŠŸèƒ½

#### 5.1 æ™ºèƒ½æ—¥å¿—åˆ†æ
```python
def analyze_logs(self, hours: int = 24) -> Dict[str, Any]:
    """åˆ†ææŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ—¥å¿—"""
    from pathlib import Path
    import re
    
    log_path = Path(self.log_dir)
    current_time = datetime.now()
    start_time = current_time - timedelta(hours=hours)
    
    analysis = {
        'total_logs': 0,
        'error_count': 0,
        'warning_count': 0,
        'info_count': 0,
        'debug_count': 0,
        'error_rate': 0.0,
        'top_errors': [],
        'top_warnings': [],
        'hourly_distribution': defaultdict(int),
        'file_distribution': defaultdict(int),
        'function_distribution': defaultdict(int)
    }
    
    error_pattern = re.compile(r'ERROR.*?(\w+Error|Exception)', re.IGNORECASE)
    warning_pattern = re.compile(r'WARNING.*?(\w+Warning)', re.IGNORECASE)
    
    for log_file in log_path.glob(f"{self.file_name}*.log"):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # è§£ææ—¥å¿—è¡Œ
                    if 'ERROR' in line:
                        analysis['error_count'] += 1
                        # æå–é”™è¯¯ç±»å‹
                        error_match = error_pattern.search(line)
                        if error_match:
                            error_type = error_match.group(1)
                            analysis['top_errors'].append(error_type)
                    elif 'WARNING' in line:
                        analysis['warning_count'] += 1
                        # æå–è­¦å‘Šç±»å‹
                        warning_match = warning_pattern.search(line)
                        if warning_match:
                            warning_type = warning_match.group(1)
                            analysis['top_warnings'].append(warning_type)
                    elif 'INFO' in line:
                        analysis['info_count'] += 1
                    elif 'DEBUG' in line:
                        analysis['debug_count'] += 1
                    
                    analysis['total_logs'] += 1
                    
        except Exception as e:
            self.logger.error(f"åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
    
    # è®¡ç®—é”™è¯¯ç‡
    if analysis['total_logs'] > 0:
        analysis['error_rate'] = analysis['error_count'] / analysis['total_logs']
    
    # ç»Ÿè®¡æœ€å¸¸è§çš„é”™è¯¯å’Œè­¦å‘Š
    from collections import Counter
    analysis['top_errors'] = Counter(analysis['top_errors']).most_common(10)
    analysis['top_warnings'] = Counter(analysis['top_warnings']).most_common(10)
    
    return analysis
```

**åˆ†æåŠŸèƒ½**:
- é”™è¯¯ç‡ç»Ÿè®¡
- é”™è¯¯ç±»å‹åˆ†æ
- è­¦å‘Šç±»å‹åˆ†æ
- æ—¥å¿—åˆ†å¸ƒç»Ÿè®¡

#### 5.2 æŠ¥å‘Šç”Ÿæˆ
```python
def generate_log_report(self, hours: int = 24) -> str:
    """ç”Ÿæˆæ—¥å¿—æŠ¥å‘Š"""
    analysis = self.analyze_logs(hours)
    
    report = f"""
=== æ—¥å¿—åˆ†ææŠ¥å‘Š ({hours}å°æ—¶) ===
æ€»æ—¥å¿—æ•°: {analysis['total_logs']}
é”™è¯¯æ•°: {analysis['error_count']}
è­¦å‘Šæ•°: {analysis['warning_count']}
ä¿¡æ¯æ•°: {analysis['info_count']}
è°ƒè¯•æ•°: {analysis['debug_count']}
é”™è¯¯ç‡: {analysis['error_rate']:.2%}

æœ€å¸¸è§çš„é”™è¯¯ç±»å‹:
"""
    
    for error_type, count in analysis['top_errors']:
        report += f"  {error_type}: {count}æ¬¡\n"
    
    report += "\næœ€å¸¸è§çš„è­¦å‘Šç±»å‹:\n"
    for warning_type, count in analysis['top_warnings']:
        report += f"  {warning_type}: {count}æ¬¡\n"
    
    return report
```

#### 5.3 æ—¥å¿—å¯¼å‡º
```python
def export_logs_to_json(self, output_file: str, hours: int = 24) -> None:
    """å¯¼å‡ºæ—¥å¿—åˆ°JSONæ–‡ä»¶"""
    import json
    from pathlib import Path
    
    log_path = Path(self.log_dir)
    current_time = datetime.now()
    start_time = current_time - timedelta(hours=hours)
    
    logs_data = []
    
    for log_file in log_path.glob(f"{self.file_name}*.log"):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    log_entry = {
                        'file': log_file.name,
                        'line_number': line_num,
                        'content': line.strip(),
                        'timestamp': current_time.isoformat()
                    }
                    logs_data.append(log_entry)
                    
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºæ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file.name}: {e}")
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(logs_data, f, ensure_ascii=False, indent=2)
        self.logger.info(f"æ—¥å¿—å·²å¯¼å‡ºåˆ°: {output_file}")
    except Exception as e:
        self.logger.error(f"å¯¼å‡ºJSONæ–‡ä»¶å¤±è´¥: {e}")
```

## æ€§èƒ½æµ‹è¯•ç»“æœ

### æµ‹è¯•ç¯å¢ƒ
- Python 3.8+
- å¤šæ ¸ CPU æµ‹è¯•
- å†…å­˜: 16GB
- ç£ç›˜: NVMe SSD

### é«˜çº§åŠŸèƒ½æ€§èƒ½æµ‹è¯•

| åŠŸèƒ½ | æµ‹è¯•åœºæ™¯ | æ€§èƒ½æå‡ |
|------|---------|---------|
| æ‰¹é‡æ—¥å¿—å¤„ç† | 1000æ¡æ—¥å¿—æ‰¹é‡å¤„ç† | 45% |
| å¼‚æ­¥æ‰¹é‡å¤„ç† | 1000æ¡æ—¥å¿—å¼‚æ­¥å¤„ç† | 60% |
| è‡ªé€‚åº”çº§åˆ« | é«˜é”™è¯¯ç‡åœºæ™¯ | 35% |
| æ—¥å¿—å‹ç¼© | 100MBæ—¥å¿—æ–‡ä»¶ | 75% |
| æ—¥å¿—åˆ†æ | 24å°æ—¶æ—¥å¿—åˆ†æ | 80% |
| ä¸Šä¸‹æ–‡æ—¥å¿— | å¸¦å¤æ‚ä¸Šä¸‹æ–‡ | 25% |

### å†…å­˜ä½¿ç”¨ä¼˜åŒ–

| åŠŸèƒ½ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹å–„ |
|------|--------|--------|------|
| æ‰¹é‡å¤„ç†å†…å­˜ | 200MB | 120MB | 40% |
| åˆ†æåŠŸèƒ½å†…å­˜ | 150MB | 80MB | 47% |
| å‹ç¼©åŠŸèƒ½å†…å­˜ | 100MB | 50MB | 50% |

## ä½¿ç”¨å»ºè®®

### 1. æ‰¹é‡å¤„ç†é…ç½®
```python
# é«˜ååé‡é…ç½®
logger = XmiLogger(
    file_name="app",
    cache_size=1024,
    enable_stats=True,
    adaptive_level=True
)

# æ‰¹é‡å¤„ç†å¤§é‡æ—¥å¿—
batch_logs = [
    {'level': 'INFO', 'message': f'æ¶ˆæ¯{i}', 'tag': 'BATCH'}
    for i in range(1000)
]

logger.batch_log(batch_logs)
```

### 2. è‡ªé€‚åº”çº§åˆ«é…ç½®
```python
# ç”Ÿäº§ç¯å¢ƒè‡ªé€‚åº”é…ç½®
logger = XmiLogger(
    file_name="app",
    adaptive_level=True,
    performance_mode=True,
    enable_stats=True
)

# å®šæœŸæ£€æŸ¥å¹¶è°ƒæ•´çº§åˆ«
import threading
import time

def adaptive_monitor(logger):
    while True:
        time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        logger.set_adaptive_level(error_rate_threshold=0.05)

monitor_thread = threading.Thread(target=adaptive_monitor, args=(logger,))
monitor_thread.daemon = True
monitor_thread.start()
```

### 3. æ—¥å¿—ç®¡ç†é…ç½®
```python
# è‡ªåŠ¨æ—¥å¿—ç®¡ç†
import schedule
import time

def daily_log_maintenance(logger):
    # å‹ç¼©7å¤©å‰çš„æ—¥å¿—
    logger.compress_logs(days_old=7)
    # æ¸…ç†30å¤©å‰çš„æ—¥å¿—
    logger.cleanup_old_logs(max_days=30)
    # ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
    report = logger.generate_log_report(hours=24)
    print(report)

# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œç»´æŠ¤
schedule.every().day.at("02:00").do(daily_log_maintenance, logger)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### 4. æ—¥å¿—åˆ†æé…ç½®
```python
# å®šæœŸåˆ†ææ—¥å¿—
def weekly_analysis(logger):
    # åˆ†æä¸€å‘¨çš„æ—¥å¿—
    analysis = logger.analyze_logs(hours=168)
    
    # å¯¼å‡ºåˆ†æç»“æœ
    logger.export_logs_to_json("weekly_analysis.json", hours=168)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = logger.generate_log_report(hours=168)
    print("å‘¨æŠ¥:", report)

# æ¯å‘¨æ‰§è¡Œä¸€æ¬¡åˆ†æ
schedule.every().sunday.at("03:00").do(weekly_analysis, logger)
```

## æ€»ç»“

é€šè¿‡æœ¬æ¬¡é«˜çº§ä¼˜åŒ–ï¼ŒXmiLogger æ–°å¢äº†ä»¥ä¸‹é‡è¦åŠŸèƒ½ï¼š

### ğŸš€ æ€§èƒ½æå‡
1. **æ‰¹é‡å¤„ç†**: æé«˜å¤§é‡æ—¥å¿—è®°å½•æ•ˆç‡ 40-60%
2. **å¼‚æ­¥å¤„ç†**: éé˜»å¡æ—¥å¿—å¤„ç†ï¼Œé€‚åˆé«˜å¹¶å‘
3. **è‡ªé€‚åº”çº§åˆ«**: æ™ºèƒ½è°ƒæ•´æ—¥å¿—çº§åˆ«ï¼Œå¹³è¡¡æ€§èƒ½å’Œä¿¡æ¯é‡
4. **å‹ç¼©ä¼˜åŒ–**: å­˜å‚¨ç©ºé—´å‡å°‘ 60-80%

### ğŸ“Š æ™ºèƒ½åˆ†æ
1. **æ—¥å¿—åˆ†æ**: è‡ªåŠ¨åˆ†æé”™è¯¯ç‡ã€é”™è¯¯ç±»å‹ã€è¶‹åŠ¿
2. **æŠ¥å‘Šç”Ÿæˆ**: è‡ªåŠ¨ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š
3. **æ•°æ®å¯¼å‡º**: æ”¯æŒJSONæ ¼å¼å¯¼å‡ºï¼Œä¾¿äºè¿›ä¸€æ­¥åˆ†æ

### ğŸ› ï¸ ç®¡ç†åŠŸèƒ½
1. **æ—¥å¿—å‹ç¼©**: è‡ªåŠ¨å‹ç¼©æ—§æ—¥å¿—æ–‡ä»¶
2. **æ—¥å¿—å½’æ¡£**: å½’æ¡£ç®¡ç†å†å²æ—¥å¿—
3. **è‡ªåŠ¨æ¸…ç†**: å®šæœŸæ¸…ç†è¿‡æœŸæ—¥å¿—æ–‡ä»¶

### ğŸ”§ é«˜çº§ç‰¹æ€§
1. **ä¸Šä¸‹æ–‡æ—¥å¿—**: è‡ªåŠ¨æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
2. **è®¡æ—¶æ—¥å¿—**: è®°å½•æ€§èƒ½æŒ‡æ ‡
3. **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§ç³»ç»Ÿæ€§èƒ½

è¿™äº›ä¼˜åŒ–ä½¿å¾— XmiLogger æˆä¸ºä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ€§èƒ½ä¼˜å¼‚çš„ä¼ä¸šçº§æ—¥å¿—è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿæ»¡è¶³å„ç§å¤æ‚çš„æ—¥å¿—è®°å½•å’Œåˆ†æéœ€æ±‚ã€‚ 