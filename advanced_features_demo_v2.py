#!/usr/bin/env python
# -*- coding:utf-8 -*-

"""
XmiLogger é«˜çº§åŠŸèƒ½æ¼”ç¤ºè„šæœ¬ v2.0
å±•ç¤ºæ™ºèƒ½æ—¥å¿—è¿‡æ»¤ã€èšåˆã€ç›‘æ§ã€åˆ†å¸ƒå¼æ”¯æŒç­‰é«˜çº§åŠŸèƒ½
"""

import gc
import os
import hashlib
import datetime
import asyncio
import time
import json
import threading
import random
from concurrent.futures import ThreadPoolExecutor
from xmi_logger import XmiLogger
from xmi_logger.advanced_features import *

def demo_advanced_features():
    """æ¼”ç¤ºé«˜çº§åŠŸèƒ½"""
    print("ğŸš€ XmiLogger é«˜çº§åŠŸèƒ½æ¼”ç¤º v2.0")
    print("=" * 60)
    
    # åˆ›å»ºæ”¯æŒé«˜çº§åŠŸèƒ½çš„æ—¥å¿—è®°å½•å™¨
    log = XmiLogger(
        "advanced_demo",
        log_dir="test_logs",
        enable_stats=True
    )
    
    # åˆå§‹åŒ–é«˜çº§åŠŸèƒ½ç»„ä»¶
    log.aggregator = LogAggregator(window_size=50, flush_interval=3.0)
    log.monitor = PerformanceMonitor()
    log.distributed_logger = DistributedLogger("node-001")
    log.security = LogSecurity()
    log.database = LogDatabase("test_logs/logs.db")
    log.stream_processor = LogStreamProcessor()
    log.analyzer = LogAnalyzer()
    log.health_checker = LogHealthChecker()
    log.backup_manager = LogBackupManager("test_logs/backups")
    log.memory_optimizer = MemoryOptimizer(max_memory_mb=256)
    log.router = LogRouter()
    log.archiver = LogArchiver("test_logs/archives")
    
    # è®¾ç½®æ™ºèƒ½è·¯ç”±
    def error_handler(log_entry):
        print(f"ğŸš¨ é”™è¯¯æ—¥å¿—è·¯ç”±: {log_entry['message']}")
        # å¯ä»¥å‘é€åˆ°é”™è¯¯ç›‘æ§ç³»ç»Ÿ
    
    def security_handler(log_entry):
        print(f"ğŸ”’ å®‰å…¨æ—¥å¿—è·¯ç”±: {log_entry['message']}")
        # å¯ä»¥å‘é€åˆ°å®‰å…¨ç›‘æ§ç³»ç»Ÿ
    
    log.router.add_route(lambda entry: entry.get('level') == 'ERROR', error_handler)
    log.router.add_route(lambda entry: 'password' in entry.get('message', '').lower(), security_handler)
    
    # æ·»åŠ æµå¤„ç†å™¨
    def add_timestamp_processor(log_entry):
        log_entry['processed_timestamp'] = time.time()
        return log_entry
    
    def add_checksum_processor(log_entry):
        message = log_entry.get('message', '')
        log_entry['checksum'] = hashlib.md5(message.encode()).hexdigest()[:8]
        return log_entry
    
    log.stream_processor.add_processor(add_timestamp_processor)
    log.stream_processor.add_processor(add_checksum_processor)
    
    print("\n=== 1. æ™ºèƒ½æ—¥å¿—åˆ†ææ¼”ç¤º ===")
    test_messages = [
        "ç”¨æˆ·ç™»å½•æˆåŠŸ",
        "æ•°æ®åº“è¿æ¥å¤±è´¥: Connection refused",
        "APIè¯·æ±‚è¶…æ—¶: HTTP 504",
        "å¯†ç éªŒè¯å¤±è´¥: Invalid credentials",
        "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: 95%",
        "SQLæ³¨å…¥æ”»å‡»æ£€æµ‹: SELECT * FROM users",
        "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
        "è®¤è¯å¤±è´¥: Unauthorized access"
    ]
    
    for message in test_messages:
        analysis = log.analyzer.analyze_log({'message': message, 'level': 'INFO'})
        print(f"æ¶ˆæ¯: {message}")
        print(f"  ä¸¥é‡ç¨‹åº¦: {analysis['severity']}")
        print(f"  ç±»åˆ«: {analysis['categories']}")
        print(f"  å»ºè®®: {analysis['suggestions']}")
        print()
    
    print("\n=== 2. åˆ†å¸ƒå¼æ—¥å¿—æ¼”ç¤º ===")
    for i in range(5):
        log_id = log.distributed_logger.get_log_id()
        log.info(f"åˆ†å¸ƒå¼æ—¥å¿—æ¶ˆæ¯ {i+1} (ID: {log_id})")
    
    print("\n=== 3. å®‰å…¨æ—¥å¿—æ¼”ç¤º ===")
    sensitive_messages = [
        "ç”¨æˆ·å¯†ç : 123456",
        "APIå¯†é’¥: sk-1234567890abcdef",
        "è®¿é—®ä»¤ç‰Œ: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9",
        "æ•°æ®åº“å¯†ç : mysecretpassword",
        "æ™®é€šæ—¥å¿—æ¶ˆæ¯"
    ]
    
    for message in sensitive_messages:
        sanitized = log.security.sanitize_message(message)
        print(f"åŸå§‹: {message}")
        print(f"æ¸…ç†å: {sanitized}")
        print()
    
    print("\n=== 4. æ€§èƒ½ç›‘æ§æ¼”ç¤º ===")
    # æ¨¡æ‹Ÿå¤§é‡æ—¥å¿—è®°å½•
    def worker(worker_id):
        for i in range(100):
            start_time = time.perf_counter()
            log.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} - æ¶ˆæ¯ {i}")
            processing_time = (time.perf_counter() - start_time) * 1000
            log.monitor.record_log("INFO", processing_time)
            time.sleep(0.001)
    
    # ä½¿ç”¨çº¿ç¨‹æ± æµ‹è¯•å¹¶å‘
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(worker, i) for i in range(5)]
        for future in futures:
            future.result()
    
    # è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = log.monitor.get_metrics()
    print("æ€§èƒ½æŒ‡æ ‡:")
    print(json.dumps(metrics, indent=2, ensure_ascii=False))
    
    print("\n=== 5. æ—¥å¿—èšåˆæ¼”ç¤º ===")
    # è®°å½•é‡å¤æ—¥å¿—
    for i in range(20):
        log.aggregator.add_log({
            'level': 'INFO',
            'message': 'é‡å¤çš„æ—¥å¿—æ¶ˆæ¯',
            'timestamp': time.time()
        })
    
    # ç­‰å¾…èšåˆå¤„ç†
    time.sleep(4)
    
    print("\n=== 6. æµå¤„ç†æ¼”ç¤º ===")
    # å¤„ç†ä¸€äº›æ—¥å¿—
    test_logs = [
        {'level': 'INFO', 'message': 'æµ‹è¯•æ¶ˆæ¯1'},
        {'level': 'ERROR', 'message': 'æµ‹è¯•é”™è¯¯1'},
        {'level': 'WARNING', 'message': 'æµ‹è¯•è­¦å‘Š1'}
    ]
    
    for log_entry in test_logs:
        log.stream_processor.process_log(log_entry)
    
    # è·å–å¤„ç†åçš„æ—¥å¿—
    time.sleep(1)
    processed_logs = []
    while True:
        processed_log = log.stream_processor.get_processed_log()
        if processed_log is None:
            break
        processed_logs.append(processed_log)
    
    print("å¤„ç†åçš„æ—¥å¿—:")
    for log_entry in processed_logs:
        print(f"  {log_entry}")
    
    print("\n=== 7. æ•°æ®åº“æ”¯æŒæ¼”ç¤º ===")
    # æ’å…¥ä¸€äº›æµ‹è¯•æ—¥å¿—
    test_db_logs = [
        {
            'timestamp': datetime.datetime.now().isoformat(),
            'level': 'INFO',
            'message': 'æ•°æ®åº“æµ‹è¯•æ¶ˆæ¯1',
            'file': 'test.py',
            'line': 100,
            'function': 'test_function',
            'process_id': os.getpid(),
            'thread_id': threading.get_ident(),
            'extra_data': {'user_id': 123, 'session_id': 'abc123'}
        },
        {
            'timestamp': datetime.datetime.now().isoformat(),
            'level': 'ERROR',
            'message': 'æ•°æ®åº“æµ‹è¯•é”™è¯¯1',
            'file': 'test.py',
            'line': 200,
            'function': 'error_function',
            'process_id': os.getpid(),
            'thread_id': threading.get_ident(),
            'extra_data': {'error_code': 500, 'request_id': 'req123'}
        }
    ]
    
    for log_entry in test_db_logs:
        log.database.insert_log(log_entry)
    
    # æŸ¥è¯¢æ—¥å¿—
    logs = log.database.query_logs({'level': 'ERROR'}, limit=10)
    print(f"æŸ¥è¯¢åˆ° {len(logs)} æ¡é”™è¯¯æ—¥å¿—")
    
    print("\n=== 8. å¥åº·æ£€æŸ¥æ¼”ç¤º ===")
    health_status = log.health_checker.check_health("test_logs")
    print("ç³»ç»Ÿå¥åº·çŠ¶æ€:")
    print(json.dumps(health_status, indent=2, ensure_ascii=False))
    
    print("\n=== 9. å¤‡ä»½ç®¡ç†æ¼”ç¤º ===")
    # åˆ›å»ºå¤‡ä»½
    backup_path = log.backup_manager.create_backup("test_logs", "demo_backup")
    print(f"åˆ›å»ºå¤‡ä»½: {backup_path}")
    
    # åˆ—å‡ºå¤‡ä»½
    backups = log.backup_manager.list_backups()
    print(f"å¤‡ä»½åˆ—è¡¨: {len(backups)} ä¸ªå¤‡ä»½")
    for backup in backups[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"  {backup['name']} - {backup['size_mb']:.2f}MB - {backup['created']}")
    
    print("\n=== 10. å†…å­˜ä¼˜åŒ–æ¼”ç¤º ===")
    # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
    large_data = []
    for i in range(10000):
        large_data.append(f"æµ‹è¯•æ•°æ® {i}" * 100)
    
    print(f"åˆ›å»ºäº† {len(large_data)} ä¸ªå¤§æ•°æ®å¯¹è±¡")
    
    # è§¦å‘å†…å­˜ä¼˜åŒ–
    if log.memory_optimizer.check_memory():
        print("æ£€æµ‹åˆ°å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œè§¦å‘ä¼˜åŒ–...")
        log.memory_optimizer.optimize_memory()
    
    # æ¸…ç†æ•°æ®
    del large_data
    gc.collect()
    
    print("\n=== 11. æ—¥å¿—å½’æ¡£æ¼”ç¤º ===")
    # åˆ›å»ºä¸€äº›æµ‹è¯•æ—¥å¿—æ–‡ä»¶
    test_log_content = "è¿™æ˜¯æµ‹è¯•æ—¥å¿—å†…å®¹\n" * 1000
    for i in range(3):
        with open(f"test_logs/test_log_{i}.log", 'w') as f:
            f.write(test_log_content)
    
    # å½’æ¡£æ—¥å¿—
    archived_files = log.archiver.archive_logs("test_logs", days_old=0)
    print(f"å½’æ¡£äº† {len(archived_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
    
    print("\n=== 12. æ™ºèƒ½è·¯ç”±æ¼”ç¤º ===")
    # æµ‹è¯•ä¸åŒç±»å‹çš„æ—¥å¿—è·¯ç”±
    test_routes = [
        {'level': 'INFO', 'message': 'æ™®é€šä¿¡æ¯æ—¥å¿—'},
        {'level': 'ERROR', 'message': 'è¿™æ˜¯ä¸€ä¸ªé”™è¯¯'},
        {'level': 'INFO', 'message': 'ç”¨æˆ·å¯†ç : secret123'},
        {'level': 'WARNING', 'message': 'æ€§èƒ½è­¦å‘Š'},
        {'level': 'ERROR', 'message': 'å¦ä¸€ä¸ªé”™è¯¯æ¶ˆæ¯'}
    ]
    
    for log_entry in test_routes:
        print(f"è·¯ç”±æ—¥å¿—: {log_entry['level']} - {log_entry['message']}")
        log.router.route_log(log_entry)
    
    print("\n=== æ¼”ç¤ºå®Œæˆ ===")
    
    # æ¸…ç†èµ„æº
    log.cleanup()
    if hasattr(log, 'aggregator'):
        log.aggregator.stop()
    
    print("âœ… æ‰€æœ‰é«˜çº§åŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")

def demo_advanced_usage():
    """æ¼”ç¤ºé«˜çº§ç”¨æ³•"""
    print("\nğŸ”§ é«˜çº§ç”¨æ³•ç¤ºä¾‹")
    print("=" * 40)
    
    # åˆ›å»ºæ”¯æŒæ‰€æœ‰é«˜çº§åŠŸèƒ½çš„æ—¥å¿—è®°å½•å™¨
    log = XmiLogger(
        "production_app",
        log_dir="production_logs",
        enable_stats=True
    )
    
    # åˆå§‹åŒ–ç»„ä»¶
    log.aggregator = LogAggregator(window_size=100, flush_interval=5.0)
    log.monitor = PerformanceMonitor()
    log.distributed_logger = DistributedLogger("prod-node-001")
    log.security = LogSecurity()
    log.database = LogDatabase("production_logs/logs.db")
    log.stream_processor = LogStreamProcessor()
    log.analyzer = LogAnalyzer()
    log.health_checker = LogHealthChecker()
    log.backup_manager = LogBackupManager("production_logs/backups")
    log.memory_optimizer = MemoryOptimizer(max_memory_mb=1024)
    log.router = LogRouter()
    log.archiver = LogArchiver("production_logs/archives")
    
    # è®¾ç½®ç”Ÿäº§ç¯å¢ƒè·¯ç”±
    def critical_error_handler(log_entry):
        # å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
        print(f"ğŸš¨ ä¸¥é‡é”™è¯¯å‘Šè­¦: {log_entry['message']}")
    
    def security_alert_handler(log_entry):
        # å‘é€åˆ°å®‰å…¨ç›‘æ§
        print(f"ğŸ”’ å®‰å…¨å‘Šè­¦: {log_entry['message']}")
    
    def performance_alert_handler(log_entry):
        # å‘é€åˆ°æ€§èƒ½ç›‘æ§
        print(f"âš¡ æ€§èƒ½å‘Šè­¦: {log_entry['message']}")
    
    log.router.add_route(lambda entry: entry.get('level') == 'CRITICAL', critical_error_handler)
    log.router.add_route(lambda entry: 'security' in entry.get('message', '').lower(), security_alert_handler)
    log.router.add_route(lambda entry: 'performance' in entry.get('message', '').lower(), performance_alert_handler)
    
    # æ·»åŠ è‡ªå®šä¹‰æµå¤„ç†å™¨
    def add_correlation_id(log_entry):
        if 'correlation_id' not in log_entry:
            log_entry['correlation_id'] = str(uuid.uuid4())[:8]
        return log_entry
    
    def add_service_name(log_entry):
        log_entry['service_name'] = 'production_app'
        return log_entry
    
    log.stream_processor.add_processor(add_correlation_id)
    log.stream_processor.add_processor(add_service_name)
    
    # æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒæ—¥å¿—
    print("æ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒæ—¥å¿—è®°å½•...")
    
    # æ­£å¸¸ä¸šåŠ¡æ—¥å¿—
    for i in range(10):
        log.info(f"ç”¨æˆ· {i+1} ç™»å½•æˆåŠŸ")
        log.info(f"å¤„ç†è®¢å• {i+1}")
        log.debug(f"æ•°æ®åº“æŸ¥è¯¢æ‰§è¡Œ: SELECT * FROM orders WHERE id = {i+1}")
    
    # é”™è¯¯æ—¥å¿—
    try:
        raise ValueError("æ¨¡æ‹Ÿçš„ä¸šåŠ¡é”™è¯¯")
    except Exception as e:
        log.error(f"ä¸šåŠ¡å¤„ç†å¤±è´¥: {e}")
    
    # å®‰å…¨ç›¸å…³æ—¥å¿—
    log.warning("æ£€æµ‹åˆ°å¯ç–‘ç™»å½•å°è¯•: IP 192.168.1.100")
    log.error("å®‰å…¨: ç”¨æˆ·å¯†ç éªŒè¯å¤±è´¥æ¬¡æ•°è¿‡å¤š")
    
    # æ€§èƒ½ç›¸å…³æ—¥å¿—
    log.warning("æ€§èƒ½: APIå“åº”æ—¶é—´è¶…è¿‡é˜ˆå€¼ (2.5s)")
    log.info("æ€§èƒ½: æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨ç‡ 85%")
    
    # è·å–ç³»ç»ŸçŠ¶æ€
    print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š:")
    metrics = log.monitor.get_metrics()
    print(f"  æ€»æ—¥å¿—æ•°: {metrics.get('log_count', 0)}")
    print(f"  é”™è¯¯æ•°: {metrics.get('error_count', 0)}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {metrics.get('avg_processing_time', 0):.2f}ms")
    print(f"  å†…å­˜ä½¿ç”¨: {metrics.get('memory_usage', 0):.2f}MB")
    print(f"  CPUä½¿ç”¨ç‡: {metrics.get('cpu_usage', 0):.2f}%")
    print(f"  ååé‡: {metrics.get('throughput', 0):.2f} æ—¥å¿—/ç§’")
    
    # å¥åº·æ£€æŸ¥
    health = log.health_checker.check_health("production_logs")
    print(f"\nğŸ¥ ç³»ç»Ÿå¥åº·çŠ¶æ€: {health['status']}")
    if health.get('warnings'):
        print("  è­¦å‘Š:")
        for warning in health['warnings']:
            print(f"    - {warning}")
    
    # åˆ›å»ºå¤‡ä»½
    backup_path = log.backup_manager.create_backup("production_logs", "daily_backup")
    print(f"\nğŸ’¾ åˆ›å»ºå¤‡ä»½: {backup_path}")
    
    print("\nâœ… é«˜çº§ç”¨æ³•æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    try:
        demo_advanced_features()
        demo_advanced_usage()
        
        print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ“‹ æ–°å¢é«˜çº§åŠŸèƒ½æ€»ç»“:")
        print("âœ… æ™ºèƒ½æ—¥å¿—åˆ†æ - è‡ªåŠ¨è¯†åˆ«é”™è¯¯ã€è­¦å‘Šã€å®‰å…¨äº‹ä»¶")
        print("âœ… åˆ†å¸ƒå¼æ—¥å¿—æ”¯æŒ - å”¯ä¸€æ—¥å¿—IDå’ŒèŠ‚ç‚¹æ ‡è¯†")
        print("âœ… æ—¥å¿—å®‰å…¨åŠŸèƒ½ - æ•æ„Ÿä¿¡æ¯æ¸…ç†å’ŒåŠ å¯†")
        print("âœ… æ€§èƒ½ç›‘æ§ - å®æ—¶ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨")
        print("âœ… æ—¥å¿—èšåˆ - è‡ªåŠ¨èšåˆé‡å¤æ—¥å¿—")
        print("âœ… æµå¤„ç† - å¯æ‰©å±•çš„æ—¥å¿—å¤„ç†ç®¡é“")
        print("âœ… æ•°æ®åº“æ”¯æŒ - ç»“æ„åŒ–æ—¥å¿—å­˜å‚¨å’ŒæŸ¥è¯¢")
        print("âœ… å¥åº·æ£€æŸ¥ - ç³»ç»ŸçŠ¶æ€ç›‘æ§")
        print("âœ… å¤‡ä»½ç®¡ç† - è‡ªåŠ¨å¤‡ä»½å’Œæ¢å¤")
        print("âœ… å†…å­˜ä¼˜åŒ– - æ™ºèƒ½åƒåœ¾å›æ”¶")
        print("âœ… æ™ºèƒ½è·¯ç”± - åŸºäºæ¡ä»¶çš„æ—¥å¿—åˆ†å‘")
        print("âœ… æ—¥å¿—å½’æ¡£ - è‡ªåŠ¨å‹ç¼©å’Œå½’æ¡£")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 